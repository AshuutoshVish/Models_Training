{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T13:31:00.099384Z","iopub.execute_input":"2025-03-27T13:31:00.099713Z","iopub.status.idle":"2025-03-27T13:31:01.560599Z","shell.execute_reply.started":"2025-03-27T13:31:00.099685Z","shell.execute_reply":"2025-03-27T13:31:01.558871Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"#  Import Libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom sklearn.metrics.pairwise import cosine_similarity","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T13:31:01.561966Z","iopub.execute_input":"2025-03-27T13:31:01.562646Z","iopub.status.idle":"2025-03-27T13:31:07.941568Z","shell.execute_reply.started":"2025-03-27T13:31:01.562597Z","shell.execute_reply":"2025-03-27T13:31:07.940673Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"\n#  Load Dataset\nfile_path = '/kaggle/input/original-hiring-dataset/Hiring_dataset.csv'\ndf = pd.read_csv(file_path)\n\n#  Preprocessing\nX = df.drop(['HiringDecision'], axis=1)\ny = df['HiringDecision']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T13:31:07.942439Z","iopub.execute_input":"2025-03-27T13:31:07.942864Z","iopub.status.idle":"2025-03-27T13:31:07.970424Z","shell.execute_reply.started":"2025-03-27T13:31:07.942837Z","shell.execute_reply":"2025-03-27T13:31:07.968870Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-1a38910554de>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#  Load Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/kaggle/input/original-hiring-dataset/Hiring_dataset.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#  Preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/original-hiring-dataset/Hiring_dataset.csv'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/original-hiring-dataset/Hiring_dataset.csv'","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"\n\n#  Identify Numerical & Categorical Features\nnumerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\ncategorical_features = X.select_dtypes(include=['object']).columns.tolist()\n\n#  Scale Numerical Features\nscaler = StandardScaler()\nX_numerical = scaler.fit_transform(X[numerical_features])\n\n#  Encode Categorical Features\nencoder = OneHotEncoder()\nX_categorical = encoder.fit_transform(X[categorical_features]).toarray()\n\n#  Combine Preprocessed Features\nX_preprocessed = np.hstack((X_numerical, X_categorical))\n\n#  Split Data\nX_train, X_test = train_test_split(X_preprocessed, test_size=0.2, random_state=42)\n\n# Convert to PyTorch tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n\n#  Deep Learning Model (Autoencoder-like)\nclass ContentBasedRecommender(nn.Module):\n    def __init__(self, input_dim, embedding_dim=64):\n        super(ContentBasedRecommender, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, embedding_dim)\n        \n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n#  Initialize Model\ninput_dim = X_train_tensor.shape[1]\nembedding_dim = 64\n\nmodel = ContentBasedRecommender(input_dim, embedding_dim)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.MSELoss()\n\n#  Contrastive Loss Function (Fixed)\ndef contrastive_loss(embeddings):\n    \"\"\"\n    Contrastive loss using cosine similarity in the embedding space.\n    \"\"\"\n    batch_size = embeddings.shape[0]\n\n    # Randomly select positive and negative samples\n    idx = torch.randperm(batch_size)\n\n    positive = embeddings\n    negative = embeddings[idx]\n\n    #  Cosine similarities\n    sim_pos = F.cosine_similarity(positive, positive)  # Similarities with itself (should be 1)\n    sim_neg = F.cosine_similarity(positive, negative)  # Similarities with random negatives\n\n    #  Contrastive loss calculation\n    loss = torch.mean(1 - sim_pos + sim_neg)\n    return loss\n\n# Training Loop\nepochs = 50\nbatch_size = 64\n\nfor epoch in range(epochs):\n    model.train()\n\n    for i in range(0, len(X_train_tensor), batch_size):\n        batch = X_train_tensor[i:i + batch_size]\n\n        optimizer.zero_grad()\n\n        # Forward pass\n        embeddings = model(batch)\n\n        # Contrastive loss (fixed)\n        loss = contrastive_loss(embeddings)\n\n        #  Backward pass\n        loss.backward()\n        optimizer.step()\n\n    if (epoch + 1) % 10 == 0:\n        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n\n# Get Embeddings\nmodel.eval()\ntrain_embeddings = model(X_train_tensor).detach().numpy()\ntest_embeddings = model(X_test_tensor).detach().numpy()\n\nprint(\"\\n Embeddings Generated Successfully!\")\nprint(\"Train Embeddings Shape:\", train_embeddings.shape)\nprint(\"Test Embeddings Shape:\", test_embeddings.shape)\n\n#  Recommendation System\ndef recommend_candidates(target_profile, top_n=5):\n    \"\"\"\n    Recommend top-N candidates based on similarity to the target profile.\n    \"\"\"\n    #  Preprocess Target Profile\n    target_numerical = scaler.transform(target_profile[numerical_features])\n    target_categorical = encoder.transform(target_profile[categorical_features]).toarray()\n    target_preprocessed = np.hstack((target_numerical, target_categorical))\n    \n    # Convert to PyTorch tensor\n    target_tensor = torch.tensor(target_preprocessed, dtype=torch.float32)\n\n    # Generate target embedding\n    with torch.no_grad():\n        target_embedding = model(target_tensor).detach().numpy()\n\n    # Calculate cosine similarities\n    similarities = cosine_similarity(target_embedding, test_embeddings)[0]\n\n    # Rank candidates by similarity\n    ranked_indices = np.argsort(similarities)[::-1]\n    top_candidates = ranked_indices[:top_n]\n\n    #  Prepare Recommendation DataFrame\n    recommendations = pd.DataFrame({\n        'Candidate_Index': top_candidates,\n        'Similarity_Score': similarities[top_candidates]\n    })\n    \n    return recommendations\n\n#  Sample Target Profile\nsample_target = pd.DataFrame({\n    'Age': [35],\n    'Gender': ['Male'],\n    'EducationLevel': [\"Master's\"],\n    'ExperienceYears': [8],\n    'PreviousCompanies': [3],\n    'DistanceFromCompany': [20],\n    'InterviewScore': [75],\n    'SkillScore': [80],\n    'PersonalityScore': [85],\n    'RecruitmentStrategy': ['Aggressive']\n})\n\n# Recommend Candidates\nrecommendations = recommend_candidates(sample_target, top_n=5)\nprint(\"\\ Top Recommended Candidates:\")\nprint(recommendations)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T13:31:07.970986Z","iopub.status.idle":"2025-03-27T13:31:07.971386Z","shell.execute_reply":"2025-03-27T13:31:07.971195Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"recommendations","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}